import numpy as np

import torch
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from link2.utils import show_image
from link2.data_preprocessing import FlickrDataset, CapsCollate
from link2.data_preprocessing import transforms
from definitions import cwd


def greedy_decoding(model, img_batched, sos_id, eos_id, pad_id, idx2word, max_len, device, tgt_mask):
    """Performs greedy decoding for the caption generation.
    At each iteration model predicts the next word in the caption given the previously
    generated words and image features. For the next word we always take the most probable one.
    Arguments:
        model (torch.nn.Module): SamLynnEvans Decoder model which generates prediction for the next word
        img_features_padded (torch.Tensor): Image features generated by CNN encoder
            Stacked along 0-th dimension for each image in the mini-batch
        sos_id (int): Id of <start> token in the vocabulary
        eos_id (int): Id of <end> token in the vocabulary
        pad_id (int): Id of <pad> token in the vocabulary
        idx2word (dict): Mapping from ordinal number of token (i.e. class number) to the string of word
        max_len (int): Maximum length of the caption
        device (torch.device): Device on which to port used tensors
    Returns:
        generated_captions (list of str): Captions generated for each image in the batch
    """
    batch_size = img_batched.size(0)

    # Define the initial state of decoder input
    x_words = torch.Tensor([sos_id] + [pad_id] * (max_len - 1)).to(device).long()
    x_words = x_words.repeat(batch_size, 1)
    padd_mask = torch.Tensor([True] * max_len).to(device).bool()
    padd_mask = padd_mask.repeat(batch_size, 1)

    # Is each image from the batch decoded
    is_decoded = [False] * batch_size
    generated_captions = []
    for _ in range(batch_size):
        generated_captions.append([])

    for i in range(max_len - 1):
        # Update the padding masks
        padd_mask[:, i] = False

        # Get the model prediction for the next word
        y_pred_prob = model(img_batched, x_words, padd_mask, tgt_mask=tgt_mask)
        # Extract the prediction from the specific (next word) position of the target sequence
        y_pred_prob  = y_pred_prob[:, i, :].clone()
        # Extract the most probable word
        y_pred = y_pred_prob.argmax(-1)

        #y_pred_prob_prev = y_pred_prob.clone()
        #y_pred_prev = y_pred.clone()

        for batch_idx in range(batch_size):
            if is_decoded[batch_idx]:
                continue
            # Add the generated word to the caption
            generated_captions[batch_idx].append(idx2word[y_pred[batch_idx].item()])
            if y_pred[batch_idx] == eos_id:
                # Caption has been fully generated for this image
                is_decoded[batch_idx] = True
            
        if np.all(is_decoded):
            break

        if i < (max_len - 1):   # We haven't reached maximum number of decoding steps
            # Update the input tokens for the next iteration
            x_words[torch.arange(batch_size), [i+1] * batch_size] = y_pred.view(-1)

    # Complete the caption for images which haven't been fully decoded
    for batch_idx in range(batch_size):
        if not is_decoded[batch_idx]:
            generated_captions[batch_idx].append(idx2word[eos_id])

    # Clean the EOS symbol
    for caption in generated_captions:
        caption.remove(idx2word[eos_id])

    return generated_captions


def idx_batch_to_sentences(idx_batch, idx2word, pad_idx):
    results = list()
    for idx_batch_line in idx_batch:
        sentence_word = []
        for idx in idx_batch_line:
            idx = idx.item()
            if idx == pad_idx:
                break
            word = idx2word[idx] if idx in idx2word.keys() else "U"
            sentence_word.append(word)
        results.append(" ".join(sentence_word))
    return results

def show_transformer_validation(model,
                                data_loader_validation,
                                tb,
                                device,
                                sos_idx,
                                eos_idx,
                                pad_idx,
                                idx2word,
                                max_len,
                                batch_size_val
                                ):
    model.eval()
    with torch.no_grad():
        dataiter = iter(data_loader_validation)
        img, orig_captions = next(dataiter)
        img = img.to(device)
        orig_caption_words = idx_batch_to_sentences(orig_captions, idx2word, pad_idx)

        # greedy_decoding(model, img_features_batched, sos_id, eos_id, pad_id, idx2word, max_len, device):
        captions_pred_batch = greedy_decoding(model, img, sos_idx, eos_idx, pad_idx, idx2word, max_len=max_len - 1,
                                              device=device)
        captions_pred_batch = captions_pred_batch[:batch_size_val]

        for i, caption in enumerate(captions_pred_batch):
            caption = ' '.join(caption) + "|" + orig_caption_words[i]
            show_image(img[i], title=f"{i}:{caption}", tb=tb)


def show_network(model_path=f"{cwd}/models/attention_model_state_20230119_191102_090.pth",
                 batch_size_val=20,
                 seq_len=30):
    model = torch.load(model_path)
    model.eval()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"device to rune on {device}")
    data_train_images_path = f"{cwd}/data/flickr8k/Flickr8kTrainImages/"
    data_train_captions = f"{cwd}/data/flickr8k/captions_train.txt"
    data_validation_images_path = f"{cwd}/data/flickr8k/Flickr8kTrainImages/"
    data_validation_captions = f"{cwd}/data/flickr8k/captions_train.txt"
    dataset_train = FlickrDataset(root_dir=data_train_images_path, captions_file=data_train_captions,
                                  transform=transforms)
    pad_idx = dataset_train.vocab.stoi["<PAD>"]
    sos_idx = dataset_train.vocab.stoi["<SOS>"]
    eos_idx = dataset_train.vocab.stoi["<EOS>"]
    idx2word = dataset_train.vocab.itos

    id_run = model_path.split("/")[-1]
    tb = SummaryWriter(log_dir=cwd + "/tensorboard/link4/transformers_val_" + id_run)

    # Validation dataset and dataloader
    dataset_validation = FlickrDataset(root_dir=data_validation_images_path, captions_file=data_validation_captions,
                                       transform=transforms, vocab=dataset_train.vocab)
    data_loader_validation = DataLoader(dataset=dataset_validation, batch_size=batch_size_val, num_workers=0,
                                        shuffle=True, collate_fn=CapsCollate(pad_idx=pad_idx, batch_first=True,
                                                                             max_len=seq_len))
    show_transformer_validation(model,
                                data_loader_validation,
                                tb,
                                device,
                                sos_idx,
                                eos_idx,
                                pad_idx,
                                idx2word,
                                seq_len,
                                batch_size_val)


if __name__ == "__main__":
    show_network()
