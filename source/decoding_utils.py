import numpy as np

import torch
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

from source.utils import show_image
from source.data_preprocessing import FlickrDataset, CapsCollate
from source.data_preprocessing import transforms
from definitions import cwd


def greedy_decoding_transformer(model, img_batched, vocab, max_len, device, tgt_mask):
    """Performs greedy decoding for the caption generation of transformer
    At each iteration model predicts the next word in the caption given the previously
    generated words and image features. For the next word we always take the most probable one.
    Arguments:
        model (torch.nn.Module): SamLynnEvans Decoder model which generates prediction for the next word
        img_batched (torch.Tensor): Image features generated by CNN encoder
            Stacked along 0-th dimension for each image in the mini-batch
        vocab: vocabulary so we can have acces to SOS, PAD, EOS, etc.
        max_len (int): Maximum length of the caption
        device (torch.device): Device on which to port used tensors
        tgt_mask: self attention for the decoder transformer
    Returns:
        generated_captions - list of indices
    """
    batch_size = img_batched.size(0)

    # Define the initial state of decoder input
    x_words = torch.Tensor([vocab.sos_idx] + [vocab.pad_idx] * (max_len - 1)).to(device).long()
    x_words = x_words.repeat(batch_size, 1)
    tgt_key_padding_mask = torch.Tensor([True] * max_len).to(device).bool()
    tgt_key_padding_mask = tgt_key_padding_mask.repeat(batch_size, 1)

    # Is each image from the batch decoded
    is_decoded = [False] * batch_size
    generated_captions = []
    for _ in range(batch_size):
        generated_captions.append([])

    y_pred_batch_idx = torch.zeros(size=(batch_size, (max_len - 1), len(vocab.itos))).to(device)

    for i in range(max_len - 1):
        # Update the padding masks
        tgt_key_padding_mask[:, i] = False

        # Get the model prediction for the next word
        y_pred_prob = model.module.forward(img_batched, x_words, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
        # Extract the prediction from the specific (next word) position of the target sequence

        y_pred_prob  = y_pred_prob[:, i, :].clone()
        y_pred_batch_idx[:, i, :] = y_pred_prob
        # Extract the most probable word
        y_pred = y_pred_prob.argmax(-1)

        for batch_idx in range(batch_size):
            if is_decoded[batch_idx]:
                continue
            # Add the generated word to the caption
            generated_captions[batch_idx].append(vocab.itos[y_pred[batch_idx].item()])
            if y_pred[batch_idx] == vocab.eos_idx:
                # Caption has been fully generated for this image
                is_decoded[batch_idx] = True
            
        if np.all(is_decoded):
            break

        if i < (max_len - 1):   # We haven't reached maximum number of decoding steps
            # Update the input tokens for the next iteration
            x_words[torch.arange(batch_size), [i+1] * batch_size] = y_pred.view(-1)

    # Complete the caption for images which haven't been fully decoded
    for batch_idx in range(batch_size):
        if not is_decoded[batch_idx]:
            generated_captions[batch_idx].append(vocab.eos)

    # Clean the EOS symbol
    for caption in generated_captions:
        caption.remove(vocab.eos)

    return generated_captions, y_pred_batch_idx

